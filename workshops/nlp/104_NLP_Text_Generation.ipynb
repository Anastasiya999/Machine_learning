{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Generation\n",
    "\n",
    "In this section we go only through n-gram model. Other methods are presented as demo of working open source examples. Learning takes too long."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-gram model\n",
    "\n",
    "In this example, we use the Wall Street Journal corpus to generate new sentences. The corpus is available as a part of NLTK library. This example is based on [Erroll Wood's work](https://github.com/errollw/gengram)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.book import *\n",
    "\n",
    "wall_street = text7.tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to clean the text up and delete all meaningless words/characters. The easiest way is to use regular expressions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "tokens = wall_street\n",
    "\n",
    "def cleanup():\n",
    "    compiled_pattern = re.compile(\"^[a-zA-Z0-9.!?]\")\n",
    "    clean = list(filter(compiled_pattern.match,tokens))\n",
    "    return clean\n",
    "tokens = cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to build ngrams. It means that we group the tokens into a list of three that are placed next to each other. You can print the ngrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_ngrams():\n",
    "    ngrams = []\n",
    "    for i in range(len(tokens)-N+1):\n",
    "        ngrams.append(tokens[i:i+N])\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to calculate the frequency of tokens in each ngram and sum if there are more than one tokens related to a ngram. There are 85826 ngrams and 54677 frequency ngrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram_freqs(ngrams):\n",
    "    counts = {}\n",
    "\n",
    "    for ngram in ngrams:\n",
    "        token_seq  = SEP.join(ngram[:-1])\n",
    "        last_token = ngram[-1]\n",
    "\n",
    "        if token_seq not in counts:\n",
    "            counts[token_seq] = {}\n",
    "\n",
    "        if last_token not in counts[token_seq]:\n",
    "            counts[token_seq][last_token] = 0\n",
    "\n",
    "        counts[token_seq][last_token] += 1;\n",
    "\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose the next word by using the most recent tokens and adds it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_word(text, N, counts):\n",
    "\n",
    "    token_seq = SEP.join(text.split()[-(N-1):]);\n",
    "    choices = counts[token_seq].items();\n",
    "\n",
    "    total = sum(weight for choice, weight in choices)\n",
    "    r = random.uniform(0, total)\n",
    "    upto = 0\n",
    "    for choice, weight in choices:\n",
    "        upto += weight;\n",
    "        if upto > r: return choice\n",
    "    assert False # should not reach here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to setup a few parameters like the windows size N, the number of sentences that we want to generate and start of the sentence that we want to generate. The sentence start string are N-1 words that exists in our ngrams list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we have managed to get to vote for a line-item veto is characterized as a fledgling in the first appropriations bill in conference will be type F safety shape a four-foot-high concrete slab with no new issues of negotiable C.D.s usually on amounts of 1 million and 30 million barrels from new fields free of tobacco smoke . In addition Upjohn is offering a fixed-rate return and the beauty of a patient suffering from Parkinson disease . The Ginnie Mae 9 issue for November delivery ended at 58.64 cents a share or 55 million after an 89.9 million pretax charge mostly related to problems under a great deal of stress most of its personal computer lines by 5 to 17 billion yen . Douglas Madison a corporate trader with Bank of Japan said 0 it might not be feasible for large-scale commercial use . Still unresolved is Sony effort to promote vehicle occupant safety in light of other important economic issues will be assumed by Chairman Jay B.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "N=3\n",
    "\n",
    "SEP=\" \"\n",
    "\n",
    "sentence_count=5\n",
    "\n",
    "ngrams = build_ngrams()\n",
    "start_seq=\"We have\"\n",
    "\n",
    "counts = ngram_freqs(ngrams)\n",
    "\n",
    "if start_seq is None: start_seq = random.choice(list(counts.keys()))\n",
    "generated = start_seq.lower();\n",
    "\n",
    "sentences = 0\n",
    "while sentences < sentence_count:\n",
    "    generated += SEP + next_word(generated, N, counts)\n",
    "    sentences += 1 if generated.endswith(('.','!', '?')) else 0\n",
    "\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
